{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon scraping.ipynb",
      "provenance": [],
      "mount_file_id": "1_rfN7JIkm95zKg4bKrHdQ3uhBC8XVl0M",
      "authorship_tag": "ABX9TyN23MLKiskjOEKaGmPvbfnT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shlederman/amazon_tracking/blob/main/amazon_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6zRgdSXTVOT",
        "outputId": "af16ed34-319c-4d59-8f0c-ef9a0893b243"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qarg8tn4MAVA"
      },
      "source": [
        "\n",
        "!pip install bs4 -q\n",
        "!pip install lxml -q\n",
        "!pip install requests -q\n",
        "\n",
        "from datetime import date\n",
        "from bs4 import BeautifulSoup \n",
        "import requests \n",
        "\n",
        "drivePath = \"/content/drive/MyDrive/Colab Notebooks/Data Files/Amazon/\"\n",
        "\n",
        "today=date.today()\n",
        "\n",
        "\n",
        "\n",
        "def main(URL):\n",
        "\n",
        "  \n",
        "#File to which output is written.  New records are appended to end of file.  \n",
        "#The today date above is incldued with the row to be able to separate the different entries.\n",
        "#Date can eventually be used to map prices changes over time.\n",
        "  outFile = open(drivePath+\"amazon tracking.csv\", \"a\") \n",
        "  \n",
        "# Needed to prevent Amazon blocking automated requests to the website\n",
        "  HEADERS = {\n",
        "    'Host': 'www.amazon.com',\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.5',\n",
        "    'Accept-Encoding': 'gzip, deflate, br',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Upgrade-Insecure-Requests': '1',\n",
        "    'TE': 'Trailers'\n",
        "  }\n",
        "  \n",
        "\n",
        "  \n",
        "  #Requests library is used to send HTTP requests and retreive HTML/XML pages from a website\n",
        "  #BeautifulSoup library is used to obtain the data from HTML/XML files.  Using parsers, the different \n",
        "  #components of the site can be identified and captured.\n",
        "  webpage = requests.get(URL, headers=HEADERS) \n",
        "  soup = BeautifulSoup(webpage.content, \"lxml\")\n",
        "\n",
        " \n",
        "  \n",
        "  try: \n",
        "    title = soup.find(\"span\",  attrs={\"id\": 'productTitle'}) \n",
        "    title_value = title.string \n",
        "    title_string = title_value .strip().replace(',', '') \n",
        "            \n",
        "  except AttributeError:\n",
        "    title_string = \"NA\"\n",
        "  \n",
        "  print(\"Product Title = \", title_string) \n",
        "\n",
        "  # saving the title in the file \n",
        " # outFile.write(f\"{title_string},\") \n",
        "\n",
        "\n",
        "  # retreiving price \n",
        "  try: \n",
        "    price = soup.find( \"span\", attrs={'id': 'priceblock_ourprice'}).string.strip().replace(',', '') \n",
        "        # we are omitting unnecessary spaces \n",
        "        # and commas form our string \n",
        "  except AttributeError: \n",
        "    price = \"NA\"\n",
        "\n",
        "  print(\"Products price = \", price) \n",
        "  \n",
        "  # saving the price in the file\n",
        "  #outFile.write(f\"{price},\") \n",
        "\n",
        "\n",
        "  # print availiblility status \n",
        "  try: \n",
        "    available = soup.find(\"div\", attrs={'id': 'availability'}) \n",
        "    available = available.find(\"span\").string.strip().replace(',', '') \n",
        "  \n",
        "  except AttributeError: \n",
        "    available = \"NA\"\n",
        "\n",
        "  print(\"Availability = \", available) \n",
        "  \n",
        "  # saving the availibility and closing the line \n",
        "  #outFile.write(f\"{available},\\n\") \n",
        "\n",
        "  # write the data to the file\n",
        "  dataLine = [f\"{today},\" , f\"{title_string},\" , f\"{price},\" , f\"{available},\\n\"]\n",
        "  outFile.writelines(dataLine)\n",
        "  \n",
        "  # closing the file \n",
        "  outFile.close()\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__': \n",
        "  # openning our url file to access URLs \n",
        "    listfile = open(drivePath+\"amazon_to_track.txt\", \"r\") \n",
        "  \n",
        "    # iterating over the urls \n",
        "    for links in listfile.readlines(): \n",
        "      main(links)\n",
        "\n",
        "    listfile.close()\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}